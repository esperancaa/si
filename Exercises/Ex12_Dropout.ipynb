{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/Users/utilizador/Documents/GitHub/si/src\")\n",
    "\n",
    "from si.neural_networks.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe Dropout implementa a técnica de regularização chamada dropout, amplamente utilizada em redes neurais. O dropout é projetado para reduzir o overfitting durante o treinamento, desativando aleatoriamente neurônios (definindo seus valores como 0) em cada iteração de treinamento. Durante a inferência (avaliação ou predição), o dropout é desativado e todos os neurônios participam da predição."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dropout:__\n",
    "Durante o treinamento, cada neurônio tem uma probabilidade definida de ser \"desativado\" (ou seja, definido como 0). Isso força a rede a aprender representações redundantes e mais robustas.\n",
    "\n",
    "__Vantagens:__\n",
    "Reduz o overfitting.\n",
    "Aumenta a generalização ao impedir que a rede dependa excessivamente de neurônios individuais.\n",
    "\n",
    "__Desvantagens:__\n",
    "Introduz aleatoriedade durante o treinamento.\n",
    "Não garante melhoria de desempenho em todos os cenários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__:\n",
    "Inicializa a camada com uma taxa de dropout definida. Valida se o valor está no intervalo válido.\n",
    "\n",
    "__forward_propagation:__\n",
    "\n",
    "Durante o treinamento:\n",
    "Gera uma máscara binária para desativar neurônios.\n",
    "Aplica o escalonamento aos neurônios restantes.\n",
    "Durante a inferência:\n",
    "Nenhuma alteração é feita no input.\n",
    "\n",
    "__backward_propagation:__\n",
    "Propaga o erro para trás apenas pelos neurônios que estavam ativos durante o forward propagation.\n",
    "\n",
    "__output_shape:__\n",
    "Retorna o formato do input, já que o dropout não altera a forma dos dados.\n",
    "\n",
    "__parameters:__\n",
    "Retorna 0, indicando que a camada de dropout não possui parâmetros treináveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, probability: float):\n",
    "        \"\"\"\n",
    "        Initializes the Dropout layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - probability (float): Dropout rate, between 0 and 1.\n",
    "        \"\"\"\n",
    "        if not (0 <= probability <= 1):\n",
    "            raise ValueError(\"Dropout probability must be between 0 and 1.\")\n",
    "        self.probability = probability\n",
    "        self.mask = None\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward_propagation(self, input: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the dropout layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - input (np.ndarray): The input array.\n",
    "        - training (bool): Whether we are in training mode or not.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: The output array after applying dropout (or unchanged input during inference).\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        if training:\n",
    "            scaling_factor = 1 / (1 - self.probability)\n",
    "            # Gera a máscara binária, onde cada neurônio tem probabilidade (1 - probability) de ser mantido\n",
    "            self.mask = np.random.binomial(1, 1 - self.probability, size=input.shape)\n",
    "            self.output = input * self.mask * scaling_factor\n",
    "        else:\n",
    "            self.output = input\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform backward propagation through the dropout layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - output_error (np.ndarray): The output error of the layer.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: The error propagated back through the layer.\n",
    "        \"\"\"\n",
    "        return output_error * self.mask # Propaga o erro pelos neurônios ativos\n",
    "\n",
    "    def output_shape(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Returns the shape of the input (dropout does not change the shape).\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: The shape of the input.\n",
    "        \"\"\"\n",
    "        return self.input.shape if self.input is not None else None\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the layer.\n",
    "        \n",
    "        Returns:\n",
    "        - int: Always 0 for the dropout layer.\n",
    "        \"\"\"\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "[[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
      " [0.15599452 0.05808361 0.86617615 0.60111501 0.70807258]\n",
      " [0.02058449 0.96990985 0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643 0.43194502 0.29122914]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184 0.45606998]]\n",
      "\n",
      "Output in Training Mode (Dropout Applied):\n",
      "[[0.74908024 0.         1.46398788 1.19731697 0.        ]\n",
      " [0.31198904 0.         0.         1.20223002 1.41614516]\n",
      " [0.04116899 0.         0.         0.42467822 0.        ]\n",
      " [0.         0.         0.         0.86389004 0.        ]\n",
      " [1.22370579 0.         0.5842893  0.73272369 0.        ]]\n",
      "\n",
      "Output in Inference Mode (No Dropout Applied):\n",
      "[[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
      " [0.15599452 0.05808361 0.86617615 0.60111501 0.70807258]\n",
      " [0.02058449 0.96990985 0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643 0.43194502 0.29122914]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184 0.45606998]]\n"
     ]
    }
   ],
   "source": [
    "# Criar uma instância da camada Dropout\n",
    "dropout_layer = Dropout(probability=0.5)\n",
    "\n",
    "# Gerar entrada aleatória\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "input_data = np.random.rand(5, 5)  # Matriz 5x5 com valores aleatórios\n",
    "\n",
    "# Propagação para frente em modo de treinamento\n",
    "output_train = dropout_layer.forward_propagation(input_data, training=True)\n",
    "\n",
    "# Propagação para frente em modo de inferência\n",
    "output_inference = dropout_layer.forward_propagation(input_data, training=False)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Input Data:\")\n",
    "print(input_data)\n",
    "print(\"\\nOutput in Training Mode (Dropout Applied):\")\n",
    "print(output_train)\n",
    "print(\"\\nOutput in Inference Mode (No Dropout Applied):\")\n",
    "print(output_inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
